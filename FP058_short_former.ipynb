{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FP058-short-former",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7a94609b74e4118bdeb44efad5a585a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3849cca7715d4f5fa8137984db922783",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d7ea07984ffd494997986ae17adf8d26",
              "IPY_MODEL_191e9efa918949229fc16b2e7f3b1cdf",
              "IPY_MODEL_6461b7b7e1a64e6babd860006ec7f769"
            ]
          }
        },
        "3849cca7715d4f5fa8137984db922783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7ea07984ffd494997986ae17adf8d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_74ef16f3f03c40d98038c440b9b3ad12",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 12%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6bc83d74d6134955a514f11b7b3ea978"
          }
        },
        "191e9efa918949229fc16b2e7f3b1cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_66b5581f90d04012bf9a83e0850831fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 15594,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1850,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3eec56ac1994869b4291fb106cde66c"
          }
        },
        "6461b7b7e1a64e6babd860006ec7f769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab7a20ccc1e14ebe9245c850a5a5814c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1850/15594 [40:29&lt;5:01:02,  1.31s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f70d11227e147d79c3ae101d572ab11"
          }
        },
        "74ef16f3f03c40d98038c440b9b3ad12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6bc83d74d6134955a514f11b7b3ea978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66b5581f90d04012bf9a83e0850831fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3eec56ac1994869b4291fb106cde66c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab7a20ccc1e14ebe9245c850a5a5814c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f70d11227e147d79c3ae101d572ab11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seven320/-Target_on_with_Arduino/blob/master/FP058_short_former.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8OhibwrnFAQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0040676b-cd9a-4ad7-8534-d85831a6536e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 24 12:24:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENV"
      ],
      "metadata": {
        "id": "edClAIyinc3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    from google.colab import output\n",
        "    # drive._mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "dLOtcpOwTh-H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "metadata": {
        "id": "g58DOQizEn_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24bb7fc2-fc16-43fd-970f-d11f11a6dc26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "d_73lUYZHSaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a325d3-b0ec-41df-d26c-d32335e04357"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "if COLAB:\n",
        "    COMPE_NAME = \"FeedbackPrize\"\n",
        "    BASE_DIR = f\"/content/drive/MyDrive/kaggle/{COMPE_NAME}\"\n",
        "    NOTEBOOK_NAME = \"FP058-short-former\"\n",
        "    VER = re.sub(r\"\\D\", \"\", NOTEBOOK_NAME)[:3]\n",
        "else:\n",
        "    BASE_DIR = \"..\""
      ],
      "metadata": {
        "id": "41fQis2MwTJs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if COLAB:\n",
        "    !pip install transformers > /dev/null\n",
        "    !pip install einops > /dev/null\n",
        "    !pip install timm > /dev/null\n",
        "    !pip install kaggle > /dev/null\n",
        "    !pip install kaggle_datasets > /dev/null\n",
        "    !pip install git+https://github.com/albumentations-team/albumentations\n",
        "    !pip install tensorflow-determinism\n",
        "    output.clear()"
      ],
      "metadata": {
        "id": "b09jV65gFc3Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import copy\n",
        "import warnings\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GroupKFold, train_test_split, KFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "import timm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig, TFAutoModel\n",
        "from typing import Optional, Tuple\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print('TF version',tf.__version__)"
      ],
      "metadata": {
        "id": "VX7ZxFBMH3mX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9814bf-92ac-48c5-d333-ff2b6eaafff6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "5mby4p-dTKF4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if COLAB:\n",
        "    INPUT_DIR = Path(os.path.join(BASE_DIR ,f\"input\"))\n",
        "    INPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    SAVE_DIR = Path(os.path.join(BASE_DIR ,f\"models/{NOTEBOOK_NAME}\"))\n",
        "    SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    OOF_DIR = Path(os.path.join(BASE_DIR, f\"oof/{NOTEBOOK_NAME}\"))\n",
        "    OOF_DIR.mkdir(exist_ok=True, parents = True)\n",
        "\n",
        "    SUB_DIR = Path(os.path.join(BASE_DIR, f\"submission/{NOTEBOOK_NAME}\"))\n",
        "    SUB_DIR.mkdir(exist_ok=True, parents = True)\n",
        "\n",
        "else:\n",
        "    INPUT_DIR = \"../input/feedback-prize-2021/train.csv\""
      ],
      "metadata": {
        "id": "ftVu3NSSUmVs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
        "\n",
        "# tokenが存在するときは使用\n",
        "LOAD_TOKENS_FROM = None\n",
        "# os.path.join(BASE_DIR, \"models/FP030-TF\")\n",
        "\n",
        "DOWNLOADED_MODEL_PATH = None\n",
        "\n",
        "LOAD_MODEL_FROM = None\n",
        "\n",
        "if DOWNLOADED_MODEL_PATH is None:\n",
        "    # DOWNLOADED_MODEL_PATH = 'allenai/longformer-base-4096'    \n",
        "    DOWNLOADED_MODEL_PATH = \"roberta-base\"\n",
        "# MODEL_NAME = 'allenai/longformer-base-4096'\n",
        "MODEL_NAME = \"roberta-base\""
      ],
      "metadata": {
        "id": "3GobI6x7KsRN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USE MULTIPLE GPUS\n",
        "if os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print('single strategy')\n",
        "else:\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print('multiple strategy')"
      ],
      "metadata": {
        "id": "Wb_phL2mKfrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7acb64c-682e-4e81-c06a-6faf0079c148"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "single strategy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
        "print('Mixed precision enabled')"
      ],
      "metadata": {
        "id": "rs3lCJndq5sJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb718fe9-fdb4-4e7d-ad1d-d2d43efd409e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mixed precision enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\n",
        "sample_df = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))"
      ],
      "metadata": {
        "id": "z50ynnGpI-a-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############\n",
        "## Utility ##\n",
        "#############\n",
        "@contextmanager\n",
        "def timer(name: str):\n",
        "    t0 = time.time()\n",
        "    print(f\"[{name}] start\")\n",
        "    yield\n",
        "    print(f\"[{name}] done - elapsed {time.time() - t0:.2f}s\")\n",
        "\n",
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    Seeds basic parameters for reproductibility of results\n",
        "    \n",
        "    Arguments:\n",
        "        seed {int} -- Number of the seed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = str(seed) # set tf seed\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "# seed_everything(42)"
      ],
      "metadata": {
        "id": "QO13XyUmI-dG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# config"
      ],
      "metadata": {
        "id": "6FXURiyonibl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "config = {'model_name': MODEL_NAME,   \n",
        "         'max_length': 1024,\n",
        "         'train_batch_size':4,\n",
        "         'valid_batch_size':16,\n",
        "         'epochs':5,\n",
        "         'learning_rate': 5e-5,\n",
        "         'max_grad_norm':10,\n",
        "         'device': 'cuda' if cuda.is_available() else 'cpu',\n",
        "         'train_fold': [0],\n",
        "         'n_fold': 5,\n",
        "         'debug': False,\n",
        "          }\n",
        "\n",
        "# THIS WILL COMPUTE VAL SCORE DURING COMMIT BUT NOT DURING SUBMIT\n",
        "COMPUTE_VAL_SCORE = True\n",
        "\n",
        "if COLAB:\n",
        "    TRAIN_FILES = Path(os.path.join(BASE_DIR, f\"input/train\"))\n",
        "else:\n",
        "    TRAIN_FILES = Path('../input/feedback-prize-2021/train')\n",
        "if COLAB:\n",
        "    TEST_FILES = Path(os.path.join(BASE_DIR, f\"input/test\"))\n",
        "else:\n",
        "    TEST_FILES = Path('../input/feedback-prize-2021/test')\n",
        "\n",
        "if len(os.listdir(TEST_FILES)) > 5:\n",
        "    COMPUTE_VAL_SCORE = False"
      ],
      "metadata": {
        "id": "PjHL5ljhY29I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train準備"
      ],
      "metadata": {
        "id": "qC5q9yN5-FZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DOWNLOADED_MODEL_PATH == 'model':\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n",
        "    config_model.save_pretrained(SAVE_DIR)\n",
        "\n",
        "    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config_model)\n",
        "    backbone.save_pretrained(SAVE_DIR)"
      ],
      "metadata": {
        "id": "AIVloWZbYitZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDS = train_df.id.unique()\n",
        "print('There are',len(IDS),'train texts.')"
      ],
      "metadata": {
        "id": "EmIY2mxerJIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10073b62-f274-4964-971c-ace01fe29d32"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 15594 train texts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAX_LEN = 1024\n",
        "MAX_LEN = 512\n",
        "\n",
        "# THE TOKENS AND ATTENTION ARRAYS\n",
        "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\n",
        "train_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
        "train_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
        "\n",
        "# THE 14 CLASSES FOR NER\n",
        "lead_b = np.zeros((len(IDS),MAX_LEN))\n",
        "lead_i = np.zeros((len(IDS),MAX_LEN))\n",
        "\n",
        "position_b = np.zeros((len(IDS),MAX_LEN))\n",
        "position_i = np.zeros((len(IDS),MAX_LEN))\n",
        "\n",
        "evidence_b = np.zeros((len(IDS),MAX_LEN))\n",
        "evidence_i = np.zeros((len(IDS),MAX_LEN))\n",
        "\n",
        "claim_b = np.zeros((len(IDS),MAX_LEN))\n",
        "claim_i = np.zeros((len(IDS),MAX_LEN))\n",
        "\n",
        "conclusion_b = np.zeros((len(IDS),MAX_LEN))\n",
        "conclusion_i = np.zeros((len(IDS),MAX_LEN))\n",
        "\n",
        "counterclaim_b = np.zeros((len(IDS),MAX_LEN))\n",
        "counterclaim_i = np.zeros((len(IDS),MAX_LEN))\n",
        "\n",
        "rebuttal_b = np.zeros((len(IDS),MAX_LEN))\n",
        "rebuttal_i = np.zeros((len(IDS),MAX_LEN))\n",
        "\n",
        "# HELPER VARIABLES\n",
        "train_lens = []\n",
        "targets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\n",
        "targets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\n",
        "target_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n",
        "             'Counterclaim':5, 'Rebuttal':6}"
      ],
      "metadata": {
        "id": "6bIYkowkrEG7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\n",
        "assert(np.sum(train_df.groupby('id')['discourse_start'].diff()<=0)==0)\n",
        "\n",
        "# FOR LOOP THROUGH EACH TRAIN TEXT\n",
        "for id_num in tqdm(range(len(IDS))):\n",
        "    if LOAD_TOKENS_FROM: break\n",
        "    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n",
        "    n = IDS[id_num]\n",
        "    name = os.path.join(BASE_DIR, f'input/train/{n}.txt')\n",
        "    txt = open(name, 'r').read()\n",
        "    train_lens.append(len(txt.split()))\n",
        "    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
        "                                   truncation=True, return_offsets_mapping=True)\n",
        "    train_tokens[id_num,] = tokens['input_ids']\n",
        "    train_attention[id_num,] = tokens['attention_mask']\n",
        "    \n",
        "    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n",
        "    offsets = tokens['offset_mapping']\n",
        "    offset_index = 0\n",
        "    df = train_df.loc[train_df.id==n]\n",
        "    for index,row in df.iterrows():\n",
        "        a = row.discourse_start\n",
        "        b = row.discourse_end\n",
        "        if offset_index>len(offsets)-1:\n",
        "            break\n",
        "        c = offsets[offset_index][0]\n",
        "        d = offsets[offset_index][1]\n",
        "        beginning = True\n",
        "        while b>c:\n",
        "            if (c>=a)&(b>=d):\n",
        "                k = target_map[row.discourse_type]\n",
        "                if beginning:\n",
        "                    targets_b[k][id_num][offset_index] = 1\n",
        "                    beginning = False\n",
        "                else:\n",
        "                    targets_i[k][id_num][offset_index] = 1\n",
        "            offset_index += 1\n",
        "            if offset_index>len(offsets)-1:\n",
        "                break\n",
        "            c = offsets[offset_index][0]\n",
        "            d = offsets[offset_index][1]"
      ],
      "metadata": {
        "id": "TefTkT_5rEVl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "a7a94609b74e4118bdeb44efad5a585a",
            "3849cca7715d4f5fa8137984db922783",
            "d7ea07984ffd494997986ae17adf8d26",
            "191e9efa918949229fc16b2e7f3b1cdf",
            "6461b7b7e1a64e6babd860006ec7f769",
            "74ef16f3f03c40d98038c440b9b3ad12",
            "6bc83d74d6134955a514f11b7b3ea978",
            "66b5581f90d04012bf9a83e0850831fe",
            "b3eec56ac1994869b4291fb106cde66c",
            "ab7a20ccc1e14ebe9245c850a5a5814c",
            "8f70d11227e147d79c3ae101d572ab11"
          ]
        },
        "outputId": "f8f3fc98-e03c-4170-88e7-4ebcd77880cc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7a94609b74e4118bdeb44efad5a585a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/15594 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-d430a71cd59d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'input/train/{n}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
            "\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_TOKENS_FROM is None:\n",
        "    plt.hist(train_lens,bins=100)\n",
        "    plt.title('Histogram of Train Word Counts',size=16)\n",
        "    plt.xlabel('Train Word Count',size=14)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GvBq5AyUrEXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_TOKENS_FROM is None:\n",
        "    targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n",
        "    for k in range(7):\n",
        "        targets[:,:,2*k] = targets_b[k]\n",
        "        targets[:,:,2*k+1] = targets_i[k]\n",
        "    targets[:,:,14] = 1-np.max(targets,axis=-1)"
      ],
      "metadata": {
        "id": "TJkjyw2mrEZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_TOKENS_FROM is None:\n",
        "    np.save(os.path.join(SAVE_DIR, f'targets_{MAX_LEN}'), targets)\n",
        "    np.save(os.path.join(SAVE_DIR, f'tokens_{MAX_LEN}'), train_tokens)\n",
        "    np.save(os.path.join(SAVE_DIR, f'attention_{MAX_LEN}'), train_attention)\n",
        "    print('Saved NER tokens')\n",
        "else:\n",
        "    targets = np.load(f'{LOAD_TOKENS_FROM}/targets_{MAX_LEN}.npy')\n",
        "    train_tokens = np.load(f'{LOAD_TOKENS_FROM}/tokens_{MAX_LEN}.npy')\n",
        "    train_attention = np.load(f'{LOAD_TOKENS_FROM}/attention_{MAX_LEN}.npy')\n",
        "    print('Loaded NER tokens')"
      ],
      "metadata": {
        "id": "4OjL8c42rEco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets.shape"
      ],
      "metadata": {
        "id": "oYHu4uVgdO-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n",
        "    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n",
        "    \n",
        "    model_config = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH) \n",
        "    backbone = TFAutoModel.from_pretrained(DOWNLOADED_MODEL_PATH, config=model_config)\n",
        "    \n",
        "    x = backbone(tokens, attention_mask=attention)\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(x[0])\n",
        "    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(lr=1e-4),\n",
        "                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n",
        "                  metrics = [tf.keras.metrics.CategoricalAccuracy()],\n",
        "                  sample_weight_mode='temporal')\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "Ky-77W1prEfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model = build_model()"
      ],
      "metadata": {
        "id": "vXrAxl6-90Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n",
        "             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}"
      ],
      "metadata": {
        "id": "he2WmKo9VN2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds(dataset='train', verbose=True, text_ids=None, preds=None):\n",
        "    all_predictions = []\n",
        "\n",
        "    for id_num in range(len(preds)):\n",
        "    \n",
        "        # GET ID\n",
        "        if (id_num%1000==0)&(verbose): \n",
        "            print(id_num,', ',end='')\n",
        "        n = text_ids[id_num]\n",
        "    \n",
        "        # GET TOKEN POSITIONS IN CHARS\n",
        "        name = os.path.join(BASE_DIR, f'input/train/{n}.txt')\n",
        "        txt = open(name, 'r').read()\n",
        "        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
        "                                   truncation=True, return_offsets_mapping=True)\n",
        "        off = tokens['offset_mapping']\n",
        "    \n",
        "        # GET WORD POSITIONS IN CHARS\n",
        "        w = []\n",
        "        blank = True\n",
        "        for i in range(len(txt)):\n",
        "            if (txt[i]!=' ')&(txt[i]!='\\n')&(txt[i]!='\\xa0')&(txt[i]!='\\x85')&(blank==True):\n",
        "                w.append(i)\n",
        "                blank=False\n",
        "            elif (txt[i]==' ')|(txt[i]=='\\n')|(txt[i]=='\\xa0')|(txt[i]=='\\x85'):\n",
        "                blank=True\n",
        "        w.append(1e6)\n",
        "            \n",
        "        # MAPPING FROM TOKENS TO WORDS\n",
        "        word_map = -1 * np.ones(MAX_LEN,dtype='int32')\n",
        "        w_i = 0\n",
        "        for i in range(len(off)):\n",
        "            if off[i][1]==0: continue\n",
        "            while off[i][0]>=w[w_i+1]: w_i += 1\n",
        "            word_map[i] = int(w_i)\n",
        "        \n",
        "        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n",
        "        ### KEY: ###\n",
        "        # 0: LEAD_B, 1: LEAD_I\n",
        "        # 2: POSITION_B, 3: POSITION_I\n",
        "        # 4: EVIDENCE_B, 5: EVIDENCE_I\n",
        "        # 6: CLAIM_B, 7: CLAIM_I\n",
        "        # 8: CONCLUSION_B, 9: CONCLUSION_I\n",
        "        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n",
        "        # 12: REBUTTAL_B, 13: REBUTTAL_I\n",
        "        # 14: NOTHING i.e. O\n",
        "        ### NOTE THESE VALUES ARE DIVIDED BY 2 IN NEXT CODE LINE\n",
        "        pred = preds[id_num,]/2.0\n",
        "    \n",
        "        i = 0\n",
        "        while i<MAX_LEN:\n",
        "            prediction = []\n",
        "            start = pred[i]\n",
        "            if start in [0,1,2,3,4,5,6,7]:\n",
        "                prediction.append(word_map[i])\n",
        "                i += 1\n",
        "                if i>=MAX_LEN: break\n",
        "                while pred[i]==start+0.5:\n",
        "                    if not word_map[i] in prediction:\n",
        "                        prediction.append(word_map[i])\n",
        "                    i += 1\n",
        "                    if i>=MAX_LEN: break\n",
        "            else:\n",
        "                i += 1\n",
        "            prediction = [x for x in prediction if x!=-1]\n",
        "            if len(prediction)>4:\n",
        "                all_predictions.append( (n, target_map_rev[int(start)], \n",
        "                                ' '.join([str(x) for x in prediction]) ) )\n",
        "                \n",
        "    # MAKE DATAFRAME\n",
        "    if len(all_predictions) == 0: # fake\n",
        "       all_predictions = [[0, \"lead\", \"1\"], [1, \"lead\", \"3\"]]\n",
        "    df = pd.DataFrame(all_predictions)\n",
        "    df.columns = ['id','class','predictionstring']\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "wFbze7G2VLRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE FROM : Rob Mulla @robikscube\n",
        "# https://www.kaggle.com/robikscube/student-writing-competition-twitch\n",
        "def calc_overlap(row):\n",
        "    \"\"\"\n",
        "    Calculates the overlap between prediction and\n",
        "    ground truth and overlap percentages used for determining\n",
        "    true positives.\n",
        "    \"\"\"\n",
        "    set_pred = set(row.predictionstring_pred.split(' '))\n",
        "    set_gt = set(row.predictionstring_gt.split(' '))\n",
        "    # Length of each and intersection\n",
        "    len_gt = len(set_gt)\n",
        "    len_pred = len(set_pred)\n",
        "    inter = len(set_gt.intersection(set_pred))\n",
        "    overlap_1 = inter / len_gt\n",
        "    overlap_2 = inter/ len_pred\n",
        "    return [overlap_1, overlap_2]\n",
        "\n",
        "def score_feedback_comp(pred_df, gt_df):\n",
        "    \"\"\"\n",
        "    A function that scores for the kaggle\n",
        "        Student Writing Competition\n",
        "        \n",
        "    Uses the steps in the evaluation page here:\n",
        "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
        "    \"\"\"\n",
        "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
        "        .reset_index(drop=True).copy()\n",
        "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
        "        .reset_index(drop=True).copy()\n",
        "    pred_df['pred_id'] = pred_df.index\n",
        "    gt_df['gt_id'] = gt_df.index\n",
        "    # Step 1. all ground truths and predictions for a given class are compared.\n",
        "    joined = pred_df.merge(gt_df,\n",
        "                           left_on=['id','class'],\n",
        "                           right_on=['id','discourse_type'],\n",
        "                           how='outer',\n",
        "                           suffixes=('_pred','_gt')\n",
        "                          )\n",
        "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
        "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
        "\n",
        "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
        "\n",
        "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
        "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
        "    # the prediction is a match and considered a true positive.\n",
        "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
        "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
        "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
        "\n",
        "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
        "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
        "    tp_pred_ids = joined.query('potential_TP') \\\n",
        "        .sort_values('max_overlap', ascending=False) \\\n",
        "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
        "\n",
        "    # 3. Any unmatched ground truths are false negatives\n",
        "    # and any unmatched predictions are false positives.\n",
        "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
        "\n",
        "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
        "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
        "\n",
        "    # Get numbers of each type\n",
        "    TP = len(tp_pred_ids)\n",
        "    FP = len(fp_pred_ids)\n",
        "    FN = len(unmatched_gt_ids)\n",
        "    #calc microf1\n",
        "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
        "    return my_f1_score"
      ],
      "metadata": {
        "id": "m760QU95VLTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_train_score(model):\n",
        "    train_idx_cp = train_idx[:len(valid_idx)]\n",
        "\n",
        "    p = model.predict([train_tokens[train_idx_cp,], train_attention[train_idx_cp,]], \n",
        "                  batch_size=config[\"valid_batch_size\"], verbose=2)\n",
        "    print('TRAIN predictions shape:',p.shape)\n",
        "    oof_preds = np.argmax(p,axis=-1)\n",
        "\n",
        "    oof = get_preds( dataset='train', verbose=True, text_ids=IDS[train_idx_cp], preds=oof_preds)\n",
        "    oof.head()\n",
        "    # TRAIN DATAFRAME\n",
        "    train = train_df.loc[train_df['id'].isin(IDS[train_idx_cp])]\n",
        "\n",
        "    f1s = []\n",
        "    CLASSES = oof['class'].unique()\n",
        "    score_detail = {}\n",
        "    print()\n",
        "    for c in CLASSES:\n",
        "        pred_df = oof.loc[oof['class']==c].copy()\n",
        "        gt_df = train.loc[train['discourse_type']==c].copy()\n",
        "        f1 = score_feedback_comp(pred_df, gt_df)\n",
        "        print(f\"{c}:{f1:.4f}\")\n",
        "        f1s.append(f1)\n",
        "        score_detail[c] = f1\n",
        "    print(f'Overall: {np.mean(f1s):.4f}')\n",
        "    print()\n",
        "    return score_detail, np.mean(f1s)\n",
        "\n",
        "def calc_oof_score(model):\n",
        "    p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n",
        "                  batch_size=config[\"valid_batch_size\"], verbose=2)\n",
        "    print('OOF predictions shape:',p.shape)\n",
        "    oof_preds = np.argmax(p,axis=-1)\n",
        "\n",
        "    oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds)\n",
        "    oof.head()\n",
        "    # VALID DATAFRAME\n",
        "    valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n",
        "\n",
        "    f1s = []\n",
        "    score_detail = {}\n",
        "    CLASSES = oof['class'].unique()\n",
        "    print()\n",
        "    for c in CLASSES:\n",
        "        pred_df = oof.loc[oof['class']==c].copy()\n",
        "        gt_df = valid.loc[valid['discourse_type']==c].copy()\n",
        "        f1 = score_feedback_comp(pred_df, gt_df)\n",
        "        print(f\"{c}:{f1:.4f}\")\n",
        "        f1s.append(f1)\n",
        "        score_detail[c] = f1\n",
        "    print(f'Overall: {np.mean(f1s):.4f}')\n",
        "    print()\n",
        "    return score_detail, np.mean(f1s)"
      ],
      "metadata": {
        "id": "vKeVdk4xvMWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# log"
      ],
      "metadata": {
        "id": "6wzqHB-VmlYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Log():\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.valid_losses = []\n",
        "        self.train_scores = []\n",
        "        self.valid_scores = []\n",
        "        self.train_score_details = []\n",
        "        self.valid_score_details = []\n",
        "\n",
        "    def add_scores(self, train_score, valid_score):\n",
        "        self.train_scores.append(train_score)\n",
        "        self.valid_scores.append(valid_score)\n",
        "\n",
        "    def add_class_scores(self, train_score_detail, valid_score_detail):\n",
        "        self.train_score_details.append(train_score_detail)\n",
        "        self.valid_score_details.append(valid_score_detail)"
      ],
      "metadata": {
        "id": "qRzTACwQ_8u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_log(log: Log, fold):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    # plt2 = plt.add_subplot(111)\n",
        "    plt.plot(\n",
        "        np.arange(len(log.train_scores)),\n",
        "        log.train_scores,\n",
        "        \"-o\",\n",
        "        label=\"Train score\",\n",
        "        color=\"#2ca02c\")\n",
        "    plt.plot(\n",
        "        np.arange(len(log.train_scores)),\n",
        "        log.valid_scores,\n",
        "        \"-o\",\n",
        "        label=\"Val score\",\n",
        "        color=\"#d62728\")\n",
        "\n",
        "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "\n",
        "    plt.ylabel(\"Loss\", size=14)\n",
        "    plt.title(f\"Fold {fold}\", size=18)\n",
        "\n",
        "    plt.legend(loc=3)\n",
        "    plt.savefig(OOF_DIR / f\"fig{fold}.png\")\n",
        "    plt.show()\n",
        "\n",
        "def show_detail_log(log: Log, fold):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt2 = plt.gca().twinx()\n",
        "\n",
        "    classes = []\n",
        "    for score_detail in log.train_score_details:\n",
        "        classes += list(score_detail.keys())\n",
        "    \n",
        "    classes = list(set(classes))\n",
        "    train_class_scores, valid_class_scores = {}, {}\n",
        "    for epoch in range(len(log.train_score_details)):\n",
        "        for c in classes:\n",
        "            if epoch == 0:\n",
        "                train_class_scores[c] = []\n",
        "                valid_class_scores[c] = []\n",
        "            train_class_scores[c].append(log.train_score_details[epoch].get(c, 0))\n",
        "            valid_class_scores[c].append(log.valid_score_details[epoch].get(c, 0))\n",
        "\n",
        "    colorlist = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\"]\n",
        "    for i, c in enumerate(classes):\n",
        "        plt2.plot(\n",
        "            np.arange(len(log.train_scores)),\n",
        "            train_class_scores[c],\n",
        "            \"-o\",\n",
        "            label=f\"{c}_train\",\n",
        "            color = colorlist[i]\n",
        "            )\n",
        "        plt2.plot(\n",
        "            np.arange(len(log.train_scores)),\n",
        "            valid_class_scores[c],\n",
        "            \"-w\",\n",
        "            linestyle=\"dashdot\",\n",
        "            label=f\"{c}_valid\",\n",
        "            color = colorlist[i]\n",
        "            )\n",
        "\n",
        "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "\n",
        "    plt.ylabel(\"Loss\", size=14)\n",
        "    plt.title(f\"Fold {fold + 1}\", size=18)\n",
        "\n",
        "    plt.legend(loc=3)\n",
        "    plt.savefig(OOF_DIR / f\"fig{fold}_detail.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gKqiDydLmkCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(42)\n",
        "kf = KFold(n_splits = config[\"n_fold\"], shuffle = True, random_state = 200033)\n",
        "train_df[\"fold\"] = 0\n",
        "if config[\"debug\"]:\n",
        "    train_df = train_df[:1000]\n",
        "    IDS = IDS[:1000]\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(IDS)):\n",
        "    train_df.loc[train_df[\"id\"].isin(IDS[valid_idx]), \"fold\"] = fold\n",
        "\n",
        "    p = targets[valid_idx]\n",
        "    target_cnts = p.sum(axis=(0,1))\n",
        "    print(target_cnts)"
      ],
      "metadata": {
        "id": "1LqXNzXdhGJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0, 1の特殊なtoken以外の部分でランダムにtokenを書き換える\n",
        "def eraser(input_tokens, p = 0.5, del_ratio = 0.03):\n",
        "    # 0.5の確率で文章中の3文字の連続した単語をランダムな単語に書き換える\n",
        "    input_tokens = copy.deepcopy(input_tokens)\n",
        "    assert input_tokens.ndim == 2\n",
        "    if input_tokens.ndim == 2:\n",
        "        batch_size, length = input_tokens.shape\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        p_1 = np.random.rand()\n",
        "        if p_1 > p:\n",
        "            continue\n",
        "        # 連続del_words削除\n",
        "        del_words = int(np.sum(input_tokens[b] > 1) * del_ratio)\n",
        "        # print(del_words)\n",
        "        while True:\n",
        "            position = np.random.randint(1, length - del_words)\n",
        "            if np.all(input_tokens[b, position:position+del_words] > 1):  \n",
        "                input_tokens[b, position:position+del_words] = np.random.randint(2, 50253, del_words)\n",
        "                break\n",
        "    return input_tokens"
      ],
      "metadata": {
        "id": "lMQ-4FZgkXns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "xMiBbk0S4LHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(config[\"n_fold\"]):\n",
        "    print(f'Fold-{fold}/{config[\"n_fold\"]}')\n",
        "    if fold not in config[\"train_fold\"]:\n",
        "        continue\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = build_model()\n",
        "\n",
        "    # LOAD MODEL\n",
        "    if LOAD_MODEL_FROM:\n",
        "        model.load_weights(f'{LOAD_MODEL_FROM}/FP{VER}_model_fold{fold}.h5')\n",
        "    # OR TRAIN MODEL\n",
        "    else:\n",
        "        log = Log()\n",
        "        LRS = [0.25e-4, 0.25e-4, 0.25e-4, 0.25e-4, 0.25e-5]\n",
        "        best_valid_score = 0\n",
        "        for epoch in range(config[\"epochs\"]):\n",
        "            print(f\"epoch {epoch + 1}/{config['epochs']}\")\n",
        "            def lrfn(epoch):\n",
        "                return LRS[epoch]\n",
        "            \n",
        "            lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
        "\n",
        "            erased_train_tokens = eraser(train_tokens, del_ratio = 0.2)\n",
        "            model.fit(x = [erased_train_tokens[train_idx,], train_attention[train_idx,]],\n",
        "                y = targets[train_idx,],\n",
        "                validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]], targets[valid_idx,]),\n",
        "                callbacks = lr_callback,\n",
        "                epochs = 1,\n",
        "                batch_size = config[\"train_batch_size\"],\n",
        "                verbose = 1\n",
        "                )    \n",
        "            LRS.pop(0)\n",
        "\n",
        "            # TRAIN score\n",
        "            train_score_detail, train_score = calc_train_score(model)\n",
        "            # OOF\n",
        "            valid_score_detail, valid_score = calc_oof_score(model)\n",
        "\n",
        "            log.add_class_scores(train_score_detail, valid_score_detail)\n",
        "            log.add_scores(train_score, valid_score)\n",
        "\n",
        "            # SAVE MODEL WEIGHTS\n",
        "            if valid_score > best_valid_score:\n",
        "                print(f\"update best score!! {best_valid_score:.4f}→{valid_score:.4f}\")\n",
        "                best_valid_score = valid_score\n",
        "                model.save_weights(os.path.join(SAVE_DIR, f'FP{VER}_model_fold{fold}.h5'))\n",
        "                print(f\"model save valid score:{valid_score:.4f}\")\n",
        "\n",
        "    show_log(log, fold)\n",
        "    show_detail_log(log, fold)\n",
        "\n",
        "    print(f\"best valid socre: {best_valid_score:.4f}\")"
      ],
      "metadata": {
        "id": "7-8U7D4L-pej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y5R2m_U93KQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KW-IX7hb3KXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gyI4eAvWir4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pGmz_T1YjTVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8bbhK7jDmqbp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}